import { useRef, useState, useEffect } from "react";
import { FaStopCircle } from "react-icons/fa";
import { IoMicCircleSharp } from "react-icons/io5";
import ChatWindow from "./ChatWindow";
import useAutoScroll from "../hooks/useAutoScroll";
import useMessageHandler from "../hooks/useMessageHandler";
import SpeechRecognition, { useSpeechRecognition } from "react-speech-recognition";

function VoiceOnlyUI({ isOpen, autoStop, messages, setMessages, showWebCam, webcamRef }) {
  const [listening, setListening] = useState(false);
  const messagesEndRef = useRef(null);
  const { sendMessage, isTyping } = useMessageHandler(setMessages, webcamRef);

  const { transcript, resetTranscript, browserSupportsSpeechRecognition } = useSpeechRecognition();

  useAutoScroll(messages, messagesEndRef);

  // ğŸ” Log browser support on mount
  useEffect(() => {
    if (!browserSupportsSpeechRecognition) {
      console.error("âŒ Browser does NOT support Speech Recognition API.");
    } else {
      console.log("âœ… Browser supports Speech Recognition API.");
    }
  }, [browserSupportsSpeechRecognition]);

  // ğŸ” Attach global error listener
  useEffect(() => {
    if ("webkitSpeechRecognition" in window) {
      const recognition = new window.webkitSpeechRecognition();
      recognition.onerror = (e) => {
        console.error("âš ï¸ SpeechRecognition error:", e.error);
      };
    }
  }, []);

  const handleMicToggle = async () => {
    if (!listening) {
      console.log("ğŸ™ï¸ Requesting microphone access...");
      try {
        await navigator.mediaDevices.getUserMedia({ audio: true }); // ensures permission prompt
        console.log("âœ… Microphone permission granted.");

        resetTranscript();
        console.log("ğŸ™ï¸ Starting listening...");
        SpeechRecognition.startListening({ continuous: true, language: "en-IN" });
        setListening(true);
      } catch (err) {
        console.error("âŒ Microphone permission denied or error:", err);
      }
    } else {
      handleStop();
    }
  };

  const handleStop = () => {
    console.log("ğŸ›‘ Stopping listening...");
    SpeechRecognition.stopListening();
    setListening(false);

    if (transcript.trim()) {
      console.log("ğŸ“ Final transcript sent:", transcript);
      sendMessage(transcript.trim());
      resetTranscript();
    } else {
      console.log("â„¹ï¸ No transcript captured.");
    }
  };

  return (
    <>
      <ChatWindow
        messages={messages}
        isOpen={isOpen}
        messagesEndRef={messagesEndRef}
        isTyping={isTyping}
        showWebCam={showWebCam}
        mode="vo"
      />

      <div className="mt-8 flex justify-center items-center gap-6">
        {!autoStop && (
          <button
            type="button"
            onClick={!isOpen ? handleMicToggle : undefined}
            disabled={isOpen}
            aria-label={listening ? "Stop listening" : "Start listening"}
            className="transition-transform hover:scale-105 focus:outline-none disabled:opacity-60"
          >
            {listening ? (
              <FaStopCircle className="size-14 text-red-600 cursor-pointer" />
            ) : (
              <IoMicCircleSharp className="size-16 text-gray-700 cursor-pointer" />
            )}
          </button>
        )}
      </div>
    </>
  );
}

export default VoiceOnlyUI;
